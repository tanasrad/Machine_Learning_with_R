{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_6_R.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNyx/LLVG6KeSzMsg7uWCuC"
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VlzORfm9sKd",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 6 - Forecasting Numeric Data. Regression Methods\n",
        "\n",
        "Lantz, B. (2019) - Machine Learning with R. Expert techniques for predicitive modeling. [p. 167 - 216]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwmQ6yx-9tKB",
        "colab_type": "text"
      },
      "source": [
        "Using real-world data and prediction tasks, this chapter gives an introduction to techniques for estimating relationships among numeric data. This includes:\n",
        "\n",
        "*   The basics of regression analysis, a set of statistical processes for modelling the size and strength of numeric relationships.\n",
        "*   How to prepare data and interpret the regression models.\n",
        "*   An overview of different techniques on how to adapt decision tree classifiers for numeric prediction tasks.\n",
        "\n",
        "**Vorwissen über decision tree classifiers ist von vorteil (chapter 5).**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHTe5-dM9wkC",
        "colab_type": "text"
      },
      "source": [
        "## Part A - Basics of regression analysis\n",
        "\n",
        "Regression analysis is amongst the most widely used methods in science and especially machine learning. In particular, it can helpt to gain insight about a set of data which can helpt to explain the past and extrapolate into the future. This method can be applied to scientific studies, such as in the fields of economics, physics or psychology, for example to quantify the relationship between a dependent variable (the value to be predicted) and one or more independent variables (the predictors), or to identify patterns that can be used to forecast future behaviour. In statistical modelling this process is known as regression analysis, which will be introduced in the following sections. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYN4DB1i9zT3",
        "colab_type": "text"
      },
      "source": [
        "### Understanding regression\n",
        "\n",
        "Recall that a line in the classical cartesian xy-plane can be defined in the form\n",
        "$$\n",
        "y=a+bx\n",
        "$$\n",
        "where $y$ indicates the dependent variable ($y$ is to be predicted) and $x$ the indipendent variable (the predictor). The slope of that line is specified by the term $b$ and indicates an increasing line if positive, and a decreasingand if negative. The intercept with the y-axis is given by the term $a$ (when $x=0$).\n",
        "\n",
        "In machine learning a similar format of that equation is used, where the purpose of the machine is to identify values of $a$ and $b$ such that the specified line is able to describe the relationship between the supplied values $x$ to the values of $y$ in the best possible way. In practice it is rarely the case that the function perfectly relates those values, so the machine quantifies this error with an additional term. Hence, the smaller that error term, the better is the function to explain the relationship between the $y$ and $x$. \n",
        "\n",
        "This chapter focuses on the most basic regression models, the so-called **linear** regression models, since they use straight lines to explain the relationships. In the case of only one independent variable ($x_1$) it is called a **simple** linear regression, in the case of two or more independent variables ($x_1, x_2, ..., x_n$) it is known as **multiple** linear regression. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr79Xdbo96jH",
        "colab_type": "text"
      },
      "source": [
        "## Simple linear regression\n",
        "\n",
        "A simple linear regression model uses a line defined by an equation in the form\n",
        "\n",
        "$$\n",
        "y = \\alpha + \\beta x\n",
        "$$\n",
        "\n",
        "to explain the relationship between a dependent variable and a single independent variable. This equation is identical to the equation described previously, beside from the Greek characters which indicate variables that are parameters of statistical functions. As stated above, those parameters (or its estimates) however can be evaluated by performing a regression analysis. Using data from the space shuttle \"Challenger\" launch in 1986 (which went terribly wrong) will give a glimpse how such an analysis can help to gain insight about the data and to test hypotheses. \n",
        "\n",
        "Hence, a regression model that demonstrates the connection between O-ring failures (the dependent variable $y$) and the outside temperature during launch (the independent variable $x$, i.e. the predictor) could predict the possibility of failure given the expected temperature at launch. Since the parameters $\\alpha$ and $\\beta$ are necessary *ingredients* to form a line through the data set, finding their values is inevitable. As usual in real-world problems, finding an exact value, meaning the line passes through every measured data point exactly, is very unlikely. Instead, the line will rather somewhat evenly \"cut\" through the data. Therefore, the resulting values for the parameters (if error > 0) are considered *parameter estimates*. The best estimates are online the ones which generate the smallest error possible. To identify the optimal paramters such that the line is closest to the data points an estimation method known as **ordinary least squares (OLS)** is applied. \n",
        "\n",
        "Note that the term *line* is meant as the solucion space of the equation $y = \\alpha + \\beta$. Values on that line, i.e. the line itself, are predictions made by the regression model. If the observed data point is below or above the predicted solution, the error is greater than 0, since the vertical distance from the prediction to the true value is > 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYQz-_18aAc8",
        "colab_type": "text"
      },
      "source": [
        "## Ordinary least squares estimation\n",
        "\n",
        "Estimating the optimal values for the parameters $\\alpha$ and $\\beta$ means finding the optimal value for the intercept $\\alpha$ and the slope $\\beta$ such that the deviation of the predicted values ($\\hat{y}$) from the actual value ($y$) is as small as possible. Statistically speaking, *errors* are referred to as **residuals**. In OLS regression, the estimated values for the parameters are chosen such that the **sum of the squared errors (SSE)** is minimal. Mathematically speaking, the goal of OLS regression is to minimise following equation: \n",
        "\n",
        "$$\n",
        "\\sum{(y_i - \\hat{y_i})^2} = \\sum{e^2_i},\n",
        "$$\n",
        "\n",
        "where $y_i$ is the actual value and $\\hat{y_i}$ the value the regression model has predicted. The difference between those values is the residual, i.e. the error, denoted as $e$. Since the errors can be positive valued (over-estimation) or negative valued (under-estimation) they being squared to eliminate the negative values and summed across all points in the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qdcjrcp681G0",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating solutions for for $\\alpha$ and $\\beta$\n",
        "\n",
        "The solution for $\\alpha$ depends on $\\beta$. Thus, the value is obtained by applying simple algebra and solving following equation:\n",
        "\n",
        "$$\n",
        "\\alpha = \\bar{y} - \\beta\\bar{x},\n",
        "$$\n",
        "\n",
        "where $\\bar{y}$ and $\\bar{x}$ denotes the mean value of y and x, respectively. Calculating the slope of the regression model requires a bit more calculus and is calculated by solving \n",
        "\n",
        "\\begin{equation}\n",
        "\\beta = \\frac{\\sum{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sum{(x_i - \\bar{x})^2}}.\n",
        "\\end{equation}\n",
        "\n",
        "By breaking up the equation into its components, it becomes evident that the slope $\\beta$ can be calculated by dividing the covariance by the variance of the independent variable:\n",
        "\n",
        "$$\n",
        "\\beta = \\frac{\\mathrm{Cov}(x,y)}{\\mathrm{Var}(x)}\n",
        "$$\n",
        "\n",
        "For the sake of convenience no proof will be given for the equation of $\\beta$, the interested reader however can consult standard statistical books if interested. Having those statements as the basis, it is no hurdle to calculate the slope and the intercept of the regression model using built-in R functions, i.e. finding the values for $\\alpha$ and $\\beta$. This will be demonstrated on the dataset `challenger.csv` from the Packt Publishing website.\n",
        "\n",
        "First, the data has to be stored in a dataframe. Note that the independent variable $x$ is named `temperature` and the dependent variable $y$ is named `distress_ct`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p2iNuiC-Agy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data from github-repository\n",
        "url <- \"https://raw.githubusercontent.com/tanasrad/Machine_Learning_with_R/master/Ch6/challenger.csv\"\n",
        "launch <- read.csv(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPHg59bNBG4J",
        "colab_type": "text"
      },
      "source": [
        "Using built-in R functions for the calculation of covariance and variance is straightforward. Keep in mind that the independent variable $x$ is `temperature` and the value to be predicted, $y$ is the independent variable `distress_ct`. Thus, $\\beta$ is manually calculated using Cov(x,y) and Var(x):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az8o5yC0BBMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1691685e-97d8-484d-c0c0-ec440a35ac13"
      },
      "source": [
        "# estimate beta manually\n",
        "b <- cov(launch$temperature, launch$distress_ct) / var(launch$temperature)\n",
        "b"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1] -0.04753968"
            ],
            "text/latex": "-0.0475396825396825",
            "text/markdown": "-0.0475396825396825",
            "text/html": [
              "-0.0475396825396825"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EHho-xWCe-M",
        "colab_type": "text"
      },
      "source": [
        "The rounded result is -0.0475. The negative slope indicates already that for increasing values in $x$, the value for $y$ will decrease. Meaning, the `distress_ct` will deacrease by a factor of 0.0475 with increasing `temperature`. \n",
        "\n",
        "Using the computed slope, the value for the intercept $\\alpha$ can be computed, also manually using built-in R functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdoR0TXeCdAu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7c5b2e91-7026-440a-a81c-75c0dd90f5e4"
      },
      "source": [
        "# estimate alpha manually\n",
        "a <- mean(launch$distress_ct) - b * mean(launch$temperature)\n",
        "a"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1] 3.698413"
            ],
            "text/latex": "3.6984126984127",
            "text/markdown": "3.6984126984127",
            "text/html": [
              "3.6984126984127"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NVX0n_PIJWm",
        "colab_type": "text"
      },
      "source": [
        "Even though calculating the values manually for $\\alpha$ and $\\beta$ is not ideal, to further understand the regression model's fit it is usefull to first learn a method for measuring the strength of a linear relationship. Afterwards, the more sophisticated way of carrying out linear regressions with the lm-function will be introduced, as well as how to apply multiple linear regressions to problems with mulitple independent varibles. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdLNpWIEKOym",
        "colab_type": "text"
      },
      "source": [
        "### Correlations\n",
        "\n",
        "Correlation coefficients are very useful to express the linear relationship between two variables. Hence, the coefficient indicates how closely their relationship follows a straight line. The best-known correlation coefficient is the **Pearson correlation coefficient**, which ranges between +1 and -1. A correlation of zero indicates no linear relationship, the maximum and minimum values indicate that the variables are perfectly correlated. Furthermore, a correlation coefficient of +1 indicates a perfect positive correlation, where the two variables behave in the same way, and a correlation coefficient of -1 in contrast is a perfect correlation, but the variables are behaving in the exact oposite way. \n",
        "\n",
        "The Pearson's correlation is defined as:\n",
        "\n",
        "$$\n",
        "\\rho_{x,y} = \\mathrm{Corr}(x,y) = \\frac{\\mathrm{Cov}(x,y)}{\\sigma_x \\sigma_y},\n",
        "$$\n",
        "\n",
        "where $\\sigma$ denotes the standard deviation of x and y, repsecitvely. Applying this formula, the correlation between the launch `temperature` and the number of O-ring `distress_ct` events can also be computed manually using built-in R functions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J2IVbaVHoy-",
        "colab_type": "text"
      },
      "source": [
        "Clearly, this isn't the most sophisticated way to calculate the values for $\\alpha$ and $\\beta$, it is nonetheless important to "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmI_LqTiWdf4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "798dfcaa-39a9-43a6-9a96-54583e69db52"
      },
      "source": [
        "# calculate the correlation of launch data manually\n",
        "r <- cov(launch$temperature, launch$distress_ct) /\n",
        "       (sd(launch$temperature) * sd(launch$distress_ct))\n",
        "r"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1] -0.5111264"
            ],
            "text/latex": "-0.511126385574056",
            "text/markdown": "-0.511126385574056",
            "text/html": [
              "-0.511126385574056"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wphJkHpWw-W",
        "colab_type": "text"
      },
      "source": [
        "Alternatively, using the R-function cor() leads, not surprisingly, to the same result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfVcn_GEIJ2h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b2a4ac52-370b-4685-f019-a3da2deeb8c7"
      },
      "source": [
        "# check with the built-in function\n",
        "cor(launch$temperature, launch$distress_ct)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1] -0.5111264"
            ],
            "text/latex": "-0.511126385574056",
            "text/markdown": "-0.511126385574056",
            "text/html": [
              "-0.511126385574056"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcBwHam3W-r0",
        "colab_type": "text"
      },
      "source": [
        "As stated at the beginning of this section, negative correlations imply a reverse relationship, i.e. an increase in the dependent variable `temperature` ($x$) is related to a decrease in `distres_ct` $(y)$. Since the value -0.5111 is halfway ot the maximum -1, this implies that there is a non-neglectiable negative linear association. One **rule of thumb** interprets correlation strength as \"weak\" if the values are between 0.1 and 0.3, \"moderate\" in the range between 0.3 and 0.5, and \"strong\" for values above 0.5. However, this is only a rule of thumb and correlation should always be interpreted in context! Nevertheless, investigating linear relationships among independent variables and the dependent variables will be important for understanding regression models on with larger numbers of predictors. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZpKZN3Ho8Dv",
        "colab_type": "text"
      },
      "source": [
        "However, the slope of the regression model can also be calculated using the correlation coefficient:\n",
        "\n",
        "$$\n",
        "\\beta = \\frac{\\mathrm{Cov}(x,y)}{\\sigma_x \\sigma_y} \\cdot \\frac{\\sigma_y}{\\sigma_x} = \\rho_{x,y} \\cdot \\frac{\\sigma_y}{\\sigma_x}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZuH5aj2WjuG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bfe75cb1-2c9f-4e07-911b-ef8f92c69464"
      },
      "source": [
        "# computing the slope (beta) manually using correlation\n",
        "r * (sd(launch$distress_ct) / sd(launch$temperature))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1] -0.04753968"
            ],
            "text/latex": "-0.0475396825396825",
            "text/markdown": "-0.0475396825396825",
            "text/html": [
              "-0.0475396825396825"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fVCwHx-h2VQ",
        "colab_type": "text"
      },
      "source": [
        "The slope of the regression model is -0.04754, were not surprisingly both ways of manually calculating the value for $\\beta$ yields the same result. \n",
        "Such simple linear regressions are commonly carried out using the built-in lm function, which was written to fit linear models. It can be used to carry out regressions, single stratum analysis of variance and analysis of covariance. Everything that was described beforehand could have been done with this lm function. Applying the function is simple, the first input `formula` asks for the specifications, which dependent variable should be modelled by which predictor. The `~` operator can be read as \"is being modelled by\". An expression of the form `y ~ model` is interpreted as `y` is modelled by a linear predictor specified symbolically by `model`. In the code below this means `distress_ct` is being modelled by `temperature`. The secont argument `data` asks to specify, from which data set the value for `y` and `x` shoult be taken from. If nothing else is specified, this function will use the default values for the remaining parameters and performing an OLS-regression . The results will be nicely structured, which is very helpful if one is calculating a regression on multiple independent variables. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax7h5pCDhu52",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "679e5678-b6dc-453d-ba21-6316e4ac4ff2"
      },
      "source": [
        "# confirming the regression line using the lm function\n",
        "model <- lm(distress_ct ~ temperature, data = launch)\n",
        "model"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Call:\n",
              "lm(formula = distress_ct ~ temperature, data = launch)\n",
              "\n",
              "Coefficients:\n",
              "(Intercept)  temperature  \n",
              "    3.69841     -0.04754  \n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBIYfnu8nuZ1",
        "colab_type": "text"
      },
      "source": [
        "The output shows the values 3.69841 for the intercept $\\alpha$ and -0.04754 for the slope $\\beta$, matching the manual calculations. If a thorough statistical analysis of the regression analysis is required, the function `summary` provides a useful summary of the entire model, including an analysis on the residuals, statistical significance of the results, the R-squared, the t and p-values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyC9vSLKkAkh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "65d4d052-a953-44d2-d34d-d53ddde4baef"
      },
      "source": [
        "summary(model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Call:\n",
              "lm(formula = distress_ct ~ temperature, data = launch)\n",
              "\n",
              "Residuals:\n",
              "    Min      1Q  Median      3Q     Max \n",
              "-0.5608 -0.3944 -0.0854  0.1056  1.8671 \n",
              "\n",
              "Coefficients:\n",
              "            Estimate Std. Error t value Pr(>|t|)   \n",
              "(Intercept)  3.69841    1.21951   3.033  0.00633 **\n",
              "temperature -0.04754    0.01744  -2.725  0.01268 * \n",
              "---\n",
              "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
              "\n",
              "Residual standard error: 0.5774 on 21 degrees of freedom\n",
              "Multiple R-squared:  0.2613,\tAdjusted R-squared:  0.2261 \n",
              "F-statistic: 7.426 on 1 and 21 DF,  p-value: 0.01268\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcCPGrnbkAPH",
        "colab_type": "text"
      },
      "source": [
        "Interpreting the summary of the regression `model` allows to conclude that the goodness-of-fit (**R-squared**) of the regression model is 0.2613, meaning that the model is able to explain 26.13% of the variance within the data. Basically, R-squared is the ratio of the variance explained by the model and the total variance of the data. Conceptually, the higher R-squared, the smaller the errors between predicted and acutal value, i.e. the closer are the actual data points to the line of the regression model. Logically, the R-squared would increase as more variables are included in the model. Therefore it's advicable to use the **adjusted R-squared** for multiple linear regressions. Another indicator for a good/poor fit is the **P-value**, which gives the probability of observing any value equal or larger than the **t-value**, which is the measure of how many standard deviations the coefficient is far away from 0. Hence, the greater the t-value, the better the model, i.e. higher indication of a relationship between the variables. The P-value for the coefficient is 0.6% which is considered as a good fit (also indicated by the **significance codes**). Hence, the lower the P-value the higher the significance of the estimate. A small value allows for the conclusion for the presence of relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS757hfyfElX",
        "colab_type": "text"
      },
      "source": [
        "As a concluding remark for this section, note that correlation does not imply causation, it rather describes the relationship between a pair of variables, yet there could be other unmeasured explanations. For the interested readers, what such false conclusions can lead to are shown on this website. Real-world cases that are highly correlated but have no causality whatsoever, known as spurious correlations. See: http://tylervigen.com/spurious-correlations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZsUKA2OD9sy",
        "colab_type": "text"
      },
      "source": [
        "## Multiple Linear Regressions\n",
        "\n",
        "The multiple linear regression is understood as an extension of simple linear regressions. Both models have the same goal, namely to estimate the values of the slope coefficients which minimize the prediction error of a linear equation. As the name suggests, mulitple linear regressions allow for additional terms for the additional independent variables (predictors). \n",
        "\n",
        "Like any model in statistics and mathematics, multiple linear regressions also come with it's strengths and weaknesses. Multiple linear regressions ar by far the most common approach for modelling numeric data, and since most real-world analyses have more than one independent variable, it is likely this model is used for most numeric prediction tasks. It is so popular, because they can be adapted to model almost any task. However, it also makes strong assumptions about the data and are only suitable for numeric data. Categorical data for example would require additional preparation. Hence, a multiple regression models are in the form:\n",
        "\n",
        "$$\n",
        "y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_i x_i + \\epsilon ,\n",
        "$$\n",
        "\n",
        "where $\\epsilon$ has been added to describe the error of the prediction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otAlOF4GHiVw",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNSCG9xGEEPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a simple multiple regression function\n",
        "reg <- function(y, x) {\n",
        "  x <- as.matrix(x)\n",
        "  x <- cbind(Intercept = 1, x)\n",
        "  b <- solve(t(x) %*% x) %*% t(x) %*% y\n",
        "  colnames(b) <- \"estimate\"\n",
        "  print(b)\n",
        "}\n",
        "\n",
        "# examine the launch data\n",
        "str(launch)\n",
        "\n",
        "# test regression model with simple linear regression\n",
        "reg(y = launch$distress_ct, x = launch[2])\n",
        "\n",
        "# use regression model with multiple regression\n",
        "reg(y = launch$distress_ct, x = launch[2:4])\n",
        "\n",
        "# confirming the multiple regression result using the lm function (not in text)\n",
        "model <- lm(distress_ct ~ temperature + field_check_pressure + flight_num, data = launch)\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}